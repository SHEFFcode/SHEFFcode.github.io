---
layout: post
title: Need for speed
description: Let's look into why some languages are considered fast, and common misconceptions
date: 2025-08-17 15:01:35 +0300
image: '/images/Need4Speed.png'
image_caption: 'Lets understand what makes a language fast [wikimedia.org](https://commons.wikimedia.org/wiki/File:Need-For-Speed-Logo-2014-2020.png)'
tags: [Technologies]
---

Have you ever heard that Java is faster than python because it is a compiled or semi compiled language, while ruby is an interpreted language? Have you ever heard that python can't really do multithreading, but then were confused as to why there is a threading module right there in python documentation. Threading that comes with a threading.Lock class no less?  Well wonder no more, as we are going to deep dive into all of that in this article.

## Compiled vs Transpiled vs Interpreted
First topic I want to touch on is that one I've heard many times in my career. That is that Java is always faster than python (or ruby or javascript for that matter) because it is a compiled language. Or at least somewhat compiled, not as compiled as C++, but more compiled than the rest.  Well that assertion is not entirely true.

First let's look at the terms so we know what we are talking about:
- Compilation refers to taking a high level human readable language and converting it into machine code.
- Transpilation refers to taking one high level human readable language and converting it into another high level human readable language.
- Interpretation refers to a virtual machine reading the human readable language directly and converting it to machine instructions as it goes.

So now that we have our definitions right, we understand that really any language can be compiled, transpiled or interpreted. There is nothing special about java or even C++ that make them compilable, all you need is a language compiler and you can compile any language.

## Approach to program execution
Now that we see that any language can take any approach to execution, why would one language pick one approach over another? For that we need to look at advantages and disadvantages of each approach. Let's first look at the terminology:

* Speed - machine code is for a specific CPU architecture (x64, arm etc) is read directly from disk to memory and into the CPU for execution.
  * More speed is clearly better.
* Error detection - are errors caught at compile time (when the compiler runs) or at runtime (when the program is already running).
  * Catching errors (null pointers, wrong types etc) at compile time before the program runs is obviously better.
* Optimization - is the code reformatted and moved around in such a way as to increase efficiency (recursion is compiled as a for loop on JVP for example).
  * More optimization means common programming mistakes are caught and reformatted by the compiler or transpiler into more efficient code.
  * The closer to the machine the final output of the compilation, the more efficient this code can become.

* Compilation
  * Advantage - speed, error and type checking at compile time, optimization for CPU architecture and OS
  * Disadvantage - lack of portability, can only run on specific architecture and operating system, potentially need to build many different versions of the same code to target different operating systems / CPU architectures. This disadvantage is somewhat mitigated by using docker and other virtualized hardware.
* Transpilation
  * Advantage
    * Being able to use language features that are not available in the base language without having to create an interpreter or VM for the language with more features (the more feature rich language gets transpiled into less feature reach language and is ran on its virtual machine)
    * Transpilation usually does transpile time error diction which can be as simple or as advanced as the language wants to be. TypeScript is a great example of it, and you can tune how much error detection you would like to have at transpiled time. 
  * Disadvantage - extra build step (you spend time transpiring from TypeScript to JavaScript for example)
    * Larger code packages - because you are translating from one language into another, specifically for more features, those features have to be implemented via the base language, and usually take up more space that if the program was written with the original language.
    * Transpilation can have bugs and errors that can lead to odd behavior of your code
    * Harder to debug, as the language you wrote is what what's being executed.
* Interpretation
  * Advantages
    * Skip the compilation step, go direct to program execution, which is great for small scripts
    * Ability to modify the program on the fly (monkey patching), you can edit the code in memory on the fly and get the new code executed while the program is running, which is not possible on compiled languages
    * Portable, a VM on any platform can interpret your code and convert it to relevant OS and CPU architecture instructions on the fly.
  * Disadvantages
    * Slower
    * Error detection at runtime only (basically crash of the program)
    * Larger size of the application (compared to compiled CPU instructions or even VM bytecode) as the actual written text files with all their formatting have to be transmitted to users for program execution.

Now that we know the various approaches to program execution, we can start to understand why various languages chose various execution approaches. For example, if I am JavaScript, I want to be able to send my scripts to the users along with my HTML via a script tag, it makes a lot of sense to run on an interpreter.  Likewise if I am python or ruby, I want people to be able to modify my source code on the fly in development and see the results on their screen without having to go through a lengthy compilation step. Not to mention how useful monkey patching a file via console is in case of a production bug (though not the best approach, it really saves your butt sometimes in a pinch).

## Virtual Machines
Another term I've heard folks throw around Java runs bytecode on a virtual machine, but python (ruby, js) is just running on an interpreter. This is not entirely true either. For example Python has handy .pyc files that you may notice when working with python packages. So while python did make the decision to be interpreted instead of compiled, it does in fact produce bytecode (thought without a lot of error checking seen in java VM for example).  This bytecode is then executed by the python virtual machine. If you wanna know about the python VM read about it [here](https://leanpub.com/insidethepythonvirtualmachine/read).

So each language technically has a virtual machine. At least JavaScript, Python and Ruby all have it.  In fact if we look at python's .pyc files, they are bytecode very similar to what Java bytecode produces. Python virtual machine then reads the bytecode, does the error detection and even optimization steps (for example 2 * 2 + 3 is compiled down to 7, cause that can be optimized during compilation stage).

Ruby also has a virtual machine called Yarv, which is essentially a bytecode interpreter similar to python, though there is no bytecode files produced in the case of ruby, they all happen in memory at runtime.  [Rubinius](https://ruby-compilers.com/rubinius/) tried to get cached bytecode output, but has not really made much progress in the ruby ecosystem as much as I can tell.

JavaScript has it's V8 engine, which has multiple components, one of which is bytecode generation via the Ignition module, which is done in memory similar to Yarv in ruby, however, there is an additional step of TurboFan component compiling the bytecode to machine code using just in time compilation before executing it. So really JS virtual machine is much more powerful than that of Ruby or Python, and is very similar to HotSpot VM for Java.

If you want to learn more about the fascinating world of V8, watch a video here:

<iframe width="560" height="315" src="https://www.youtube.com/embed/r5OWCtuKiAk?si=untu9rHLnR3M2HhF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<br/>
### So what gives Java Speed?
Java VM is actually pretty similar to python VM, though it supports more features.  At its core it generates bytecode files (.class files) that can be executed by the java VM.  So the speed secret is really not found in the bytecode per se, but in another 2 areas:

1. JIT Compiler - as we saw above with JS, compiling human readable code to machine instructions on the fly will give you much more speed in the longer term.
2. Garbage Collector - Java VM has a much more powerful garbage collector that we will dig into later
3. GIL - global interpreter lock, a term I am sure you have heard before.

Let's examine these and try to understand how the future (and even present!) for python may look like if it wants to speed up!

## GIL or can Python thread?
One of the main dings people throw at python as compared to java is that it can't really mutli-thread. And yet when we look at the documentation we do see a whole threading module complete with a threading.Lock class. So what's the deal, can python thread or not?

### Terminology

To answer this question we must look at the terminology, starting with what is a thread is. A thread is an execution context (often with it's own stack frame) that is a part of a unix process while sharing memory between threads (something that cannot be easily done between processes).

There are 2 different types of threads:
1. Kernel threads - managed by the operating system and pre-emotively multitasked. Usually one thread per core. 
2. User threads - managed by the application, allowing more threads than cores. These are usually called green threads. These are also pre-emptively scheduled.
   1. Fibers are even lighter than user threads, but require cooperative scheduling and yielding, making them more tricky to use.

Now let's look at what a thread can help you achieve:
1. Concurrency - waiting together
2. Parallelism - working together

### Methodology
The good and the bad thing about threads is that they share memory. It's good because they can be much more light weight and can access shared memory and operate on shared objects. The bad thing is that doing so can leads to race conditions where shared mutable state is modified. This is the dreaded quadrant of hell:

![Quadrant of Hell](/images/QuadrantOfHell.png)

[Source](https://imgur.com/a/quadrant-of-JWkjMMn)

So what if we could just never land in the quadrant of hell? What if we just avoided shared mutable state? Well the issue is that this is actually really hard to do. You have two options:

1. You have to look through all of your code, and all of your underlying library dependencies, and ensure that there is no shared mutable state.
2. Not have multiple threads running at a time.

Java went with approach number 1, opting for parallelism and allowing for CPU work to be done in parallel threads on the same process at the same time.  Because of it java concurrency is one of the harder aspects of the language, and trips up even the most seasoned Java programmers.  Java was able to create a VM that can be threaded because they had very smart people and a lot of money to invest into building one as part of Sun Microsystems.

Python (and ruby, and javascript) went with approach 1. They set up a global interpreter lock, essentially only one thread can hold a lock on the language interpreter at a time, which means there is never actually really truly any shared state. Each thread aquires a lock on the interpreter and runs for a bit before the lock being taken away and given to another thread.

This reduces code complexity, but leaves threading to only serve the role of concurrency (waiting together) and not parallelism (working together). This means that all python threading module does is allows you to work with various IO sources (HTTP requests, reading files etc), but not do any heave computer work.

For that python (and ruby and js) have the concept of spinning of side workers (essentially processes with their own memory) to do the work and then return the result to the originating (parent) process.

Alternatively in a web server environment, one would use a queue (RabbitMQ + Celery for python or Redis + Sidekiq for ruby) to achieve parallel processing.

### Removing GIL
What if we tried to remove GIL from python, and give it true parallelism? Well nothing is stopping you if you only run python code. That means if you don't rely on C Extensions like Numpy and Pandas, or [Py4J](https://medium.com/@saaayush646/understanding-py4j-in-apache-spark-a4ee298f648f) projects like PySpark then you can definitely remove the GIL and manage mutable shared state via the locks provided in the concurrency package. (This will require swapping garbage collection methodology as will be described later, but it's possible.)

However, converting these C libraries to be thread safe in C python execution context has been a huge challenge, and one of the main reasons why GIL is still in place for cPython.  Making extensions that allow you to use C libraries from inside python is what made python such a popular language, and GIL made it very easy to integrate a CLibrary into python.  In many ways GIL is the reason python is so popular.

If you want to read an in detail proposal on what it would take to remove the GIL, you can read it [here](https://peps.python.org/pep-0703/).

If you want to read up on the c-code for gil itself, you can find it in the [ceval.c](https://github.com/python/cpython/blob/main/Python/ceval.c#L76-L131) file and [ceval.h](https://github.com/python/cpython/blob/58a9133fc2caea15d11116f0b5bd6832374cb88c/Include/ceval.h#L119) files.

There is also a really nice youtube video on the GIL history and abilities in python that you can watch here:

<iframe width="560" height="315" src="https://www.youtube.com/embed/KVKufdTphKs?si=-dqwlYUq1hvVZCpw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br/>

### Thread safety without GIL
If you want to keep thread safety without the gil, you have to make sure that all the data structures as safe to use between threads. You can do this via locking, via optimistic locking or through atomic data structures. All of these would have to be researched and developed in python, but will roughly be equivalent to what you can find in java (like atomicInteger, ConcurrentHashMap, CopyOnWriteArrayList etc).

## JIT Compiler
Now let's dig into the JIT compiler bit that makes Java Fast. HotSpot virtual machine was an engineering marvel when it was first released almost 25 years ago.  There was work to improve it that ended up being the GraalVm released in 2019.  Both take the Java bytecode interpreter to another level, allowing it to compile down to machine code of the architecture and OS it's running on as the java app was running, and find hot spots of bytecode that had to be interpreted again and again and compile them down to machine code on the fly.

This JIT compilation is actually what makes NodeJS so fast as well. In fact if you look at language speed charts, JavaScript today beats out almost every other language thanks to it's incredible V8 engine I talked about above. The inspiration for V8 was the Java HotSpot VM.

Take a look at this unscientific, but supported from many sources speed chart from wikipedia:

![Language Speed Chart](/images/LanguageSpeedChart.svg)
[Source](https://upload.wikimedia.org/wikipedia/commons/d/db/Barplot_language_speeds_%28Benchmarks_Game_Mandelbrot%29.svg)

I saw many of these online that indicate an incredible performance for JavaScript on NodeJs platform, that rivals that of Java on HotSpot VM.  But when we look at Ruby and Python, we see them lagging behind. So why not add a JIT compiler to them?

### JIT Comes to Python
Indeed, why not. In fact we now [have](https://peps.python.org/pep-0744/) a JIT compiler for python (and ruby!).  While [initial reports](https://tonybaloney.github.io/posts/python-gets-a-jit.html) indicate about a 10% performance increase from this initial version, we can expect to see a larger speed up and performance improvements soon! 

Keep in mind if you are using Pandas or NumPy, you are already using a compiled C code, and that is precisely the reason why those libraries are so fast in python. So really the JIT compiler will only help with the other non C code in your python programs.

Ruby got it's [JIT compiler](https://shopify.engineering/ruby-yjit-is-production-ready) in 2023 courtesy of shopify. And they saw a roughly 20% performance improvement in their testing.

## Garbage collection
The last component to give python the speed we need is garbage collection. Currently python uses 2 types of garbage collection strategies:
1. Reference Counting (as variables on the stack refer to objects in the heap, their reference count is incremented, once the stack reference goes away, the count is decremented until it reaches 0, at which point the object is de-allocated from the heap).
2. Generational garbage collection
   *  Which surprise surprise has 2 modes!
      * Default - relies on GIL for thread safety, 3 generation generational collector
      * Free threaded - stops all running threads and runs the GC on a separate one, which has no generations, but scans the shole heap

If you want to learn more about python garbage collection, feel free to read about it [here](https://github.com/python/cpython/blob/main/InternalDocs/garbage_collector.md).

The main point to take out of here, is that reference counting approach as it's primary approach to garbage collection in python. This approach currently relies on the GIL to make sure that reference counts do not change during the reference counting operation.  However using the free threaded garbage collector (which is default if gil is disabled) solves that issue to a certain extent, but is still fairly new having only been shipped on python 3.13. 


## Final thoughts
It's important to understand that the speed of a language does not depend on it's synthax, but on the underlying memory management and compilation properties of it's virtual machine (if it's not right away compiled to machine code).

What makes Node fast is not the presence of JavaScript, but rather it's absence, as it's all compiled to machine code at runtime by the V8 engine.  This in turn is directly related to the amount of time the Chrome team spent building the engine, which is directly proportional to the amount of money google had to pay them to develop it, which in turn is directly proportional to the ad revenue a faster browser is able to deliver to Google.

The reason python is single threaded and (relatively) slow (without c-extensions like numpy) and has a GIL and has not had a JIT compiler is because it did not have a large team building out these theoretically possible features. It was built by Guido von Rossum at [CWI](https://www.cwi.nl/en/) a non profit research center for mathematics in the netherlands. They needed to build a free and open source language that other researchers in the field of applied mathematics could use to solve their problems in a way that did not require a license or a computer science degree. Python did this very well.

Ruby is in a similar boat, where a single person (Yukihiro Matsumoto) needed a language that was similar to Python, but with a bit of Lisp and Smalltalk mixed in. He wanted an object oriented scripting language, and he built an excellent one. It's real claim to fame was it's use in the rails framework (if you wanna know why DHH picked it, read about it [here](https://corecursive.com/045-david-heinemeier-hansson-software-contrarian/)), and the development it received (JIT, better garbage collection etc, fibers etc) largely owe themselves to the development and popularity of rails amongst big organizations (like shopify and heroku) who could fund it's development.

So why does C++ and Java have this awesome memory model and supports threading? Because they have been developed by really smart people who had time and money to develop them while they were working at very large and profitable (at the time) organizations like ATT and Sun Microsystems.  And even these really smart folks managed to create languages whose syntax is frustrating and wordy, and threading concepts notoriously hard to manage and easy to mess up. 



## Sources
* https://dev.to/imsushant12/understanding-python-bytecode-and-the-virtual-machine-for-better-development-55a9
* https://medium.com/dailyjs/understanding-v8s-bytecode-317d46c94775
* https://v8.dev/blog/ignition-interpreter
* https://github.com/python/cpython/blob/main/InternalDocs/garbage_collector.md
* https://docs.python.org/3/library/gc.html
* https://devguide.python.org/internals/garbage-collector/index.html#collecting-the-oldest-generation
* https://corecursive.com/045-david-heinemeier-hansson-software-contrarian/
* https://en.wikipedia.org/wiki/Java_(software_platform)#History
* https://www.speedshop.co/2020/05/11/the-ruby-gvl-and-scaling.html
* https://www.youtube.com/watch?v=KVKufdTphKs&ab_channel=AlphaVideoIreland
* Ryan Dahl on NodeJS - https://www.youtube.com/watch?v=M-sc73Y-zQA&ab_channel=YUILibrary
* One of the most epic groupings of people in the JS world: https://www.youtube.com/watch?v=wxDBF3OOaRA&ab_channel=BrazilJS

